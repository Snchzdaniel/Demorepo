{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UENZx1qpoAJi",
        "outputId": "5cc3efad-a498-477c-f354-5a02359589cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.29.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.11/dist-packages (7.34.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.24.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.169.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.4)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.13.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.10.0 (from gradio)\n",
            "  Downloading gradio_client-1.10.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.31.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Collecting jedi>=0.16 (from IPython)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from IPython) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from IPython) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from IPython) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from IPython) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from IPython) (2.19.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from IPython) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from IPython) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from IPython) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->IPython) (0.8.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->IPython) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython) (0.2.13)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading gradio-5.29.0-py3-none-any.whl (54.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.1/54.1 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.10.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.9/322.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, jedi, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-24.1.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.29.0 gradio-client-1.10.0 groovy-0.1.2 jedi-0.19.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.9 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 uvicorn-0.34.2\n"
          ]
        }
      ],
      "source": [
        "# 1. Instalación de librerías necesarias\n",
        "!pip install pandas numpy matplotlib seaborn google-generativeai gradio scikit-learn tensorflow IPython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NR2L-0FpoAug",
        "outputId": "187ed234-f712-491e-f175-9ec706305ff3"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://bf14816e8aad327211.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://bf14816e8aad327211.gradio.live\" width=\"100%\" height=\"1200\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.6771 - loss: 0.6251 - precision: 0.2134 - recall: 0.0657 - val_accuracy: 0.6938 - val_loss: 0.6496 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7338 - loss: 0.5799 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.6938 - val_loss: 0.6602 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7184 - loss: 0.5957 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.6938 - val_loss: 0.6514 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7260 - loss: 0.5907 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.6938 - val_loss: 0.6491 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 5/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7169 - loss: 0.5906 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.6938 - val_loss: 0.6432 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 6/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6973 - loss: 0.6026 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.6938 - val_loss: 0.6470 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 7/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7294 - loss: 0.5729 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.6938 - val_loss: 0.6575 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 8/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7070 - loss: 0.5861 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.6938 - val_loss: 0.6572 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 9/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7180 - loss: 0.5883 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.6938 - val_loss: 0.6605 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 10/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7443 - loss: 0.5635 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.6938 - val_loss: 0.6536 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 11/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7326 - loss: 0.5658 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.6938 - val_loss: 0.6507 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 12/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7210 - loss: 0.5789 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.6938 - val_loss: 0.6480 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 13/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7453 - loss: 0.5621 - precision: 0.4762 - recall: 0.0034 - val_accuracy: 0.6938 - val_loss: 0.6566 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 14/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7116 - loss: 0.5865 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.6938 - val_loss: 0.6547 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 15/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7199 - loss: 0.5739 - precision: 0.1905 - recall: 0.0012 - val_accuracy: 0.6938 - val_loss: 0.6582 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "Epoch 1/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.5359 - loss: 0.6897 - precision_1: 0.2819 - recall_1: 0.4954 - val_accuracy: 0.6938 - val_loss: 0.6172 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 2/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7247 - loss: 0.6196 - precision_1: 0.5824 - recall_1: 0.0499 - val_accuracy: 0.6938 - val_loss: 0.6100 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 3/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7186 - loss: 0.6093 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.6938 - val_loss: 0.6140 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 4/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7368 - loss: 0.5815 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.6938 - val_loss: 0.6186 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 5/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7219 - loss: 0.5946 - precision_1: 0.4048 - recall_1: 0.0082 - val_accuracy: 0.6938 - val_loss: 0.6205 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 6/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7173 - loss: 0.5870 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.6938 - val_loss: 0.6239 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 7/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7173 - loss: 0.5885 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.6938 - val_loss: 0.6259 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 8/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7472 - loss: 0.5767 - precision_1: 0.8571 - recall_1: 0.0106 - val_accuracy: 0.6938 - val_loss: 0.6278 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 9/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7418 - loss: 0.5634 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.6938 - val_loss: 0.6325 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 10/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7401 - loss: 0.5740 - precision_1: 0.7619 - recall_1: 0.0077 - val_accuracy: 0.6938 - val_loss: 0.6343 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 11/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7323 - loss: 0.5705 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.6938 - val_loss: 0.6344 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 12/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7202 - loss: 0.5719 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.6938 - val_loss: 0.6353 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "Epoch 1/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6946 - loss: 0.6400 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.6938 - val_loss: 0.6357 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7341 - loss: 0.5988 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.6938 - val_loss: 0.6373 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6949 - loss: 0.6261 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.6938 - val_loss: 0.6386 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7096 - loss: 0.6209 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.6938 - val_loss: 0.6373 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 5/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7136 - loss: 0.6015 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.6938 - val_loss: 0.6337 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 6/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7082 - loss: 0.5969 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.6938 - val_loss: 0.6352 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 7/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7240 - loss: 0.5739 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.6938 - val_loss: 0.6414 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 8/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7318 - loss: 0.5672 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.6938 - val_loss: 0.6431 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 9/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7298 - loss: 0.5711 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.6938 - val_loss: 0.6411 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 10/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7536 - loss: 0.5501 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.6938 - val_loss: 0.6406 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 11/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7239 - loss: 0.5817 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.6938 - val_loss: 0.6392 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 12/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7029 - loss: 0.6007 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.6938 - val_loss: 0.6395 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 13/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7431 - loss: 0.5515 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.6938 - val_loss: 0.6441 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 14/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7394 - loss: 0.5613 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.6938 - val_loss: 0.6460 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 15/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7167 - loss: 0.5792 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.6938 - val_loss: 0.6479 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n"
          ]
        }
      ],
      "source": [
        "# Predicción del Riesgo Cardiovascular en Conductores de Transporte Público\n",
        "# usando Inteligencia Artificial\n",
        "\n",
        "# 1. Importación de librerías\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "import os\n",
        "import io\n",
        "import gradio as gr\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Intentar importar la API de Gemini, con manejo de errores en caso de que no esté disponible\n",
        "try:\n",
        "    import google.generativeai as genai\n",
        "    GEMINI_AVAILABLE = True\n",
        "except ImportError:\n",
        "    GEMINI_AVAILABLE = False\n",
        "    print(\"Google Generative AI no está disponible. Se simulará la funcionalidad de Gemini.\")\n",
        "\n",
        "# Variables globales\n",
        "df_global = None\n",
        "model_global = None\n",
        "scaler_global = None\n",
        "history_global = None\n",
        "feature_importance_global = None\n",
        "evaluation_global = None\n",
        "\n",
        "# Configurar estilo de visualización\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "# 2. Funciones para cargar y procesar datos\n",
        "def cargar_datos(archivo):\n",
        "    \"\"\"Carga los datos desde un archivo CSV\"\"\"\n",
        "    global df_global\n",
        "    try:\n",
        "        df = pd.read_csv(archivo)\n",
        "        df_global = df.copy()\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error al cargar el archivo: {e}\")\n",
        "        return None\n",
        "\n",
        "def preprocesar_datos(df):\n",
        "    \"\"\"Preprocesa los datos para el entrenamiento\"\"\"\n",
        "    global scaler_global\n",
        "\n",
        "    # Manejar la columna 'Fuma' (puede contener 'Sí' con acento)\n",
        "    df['Fuma'] = df['Fuma'].map({'Sí': 1, 'SÃ­': 1, 'No': 0, 'Si': 1})\n",
        "\n",
        "    # Manejar la columna 'Genero'\n",
        "    df['Genero'] = df['Genero'].map({'M': 1, 'F': 0})\n",
        "\n",
        "    # Definir características (X) y etiquetas (y)\n",
        "    X = df.drop('Riesgo_Cardiovascular', axis=1)\n",
        "    y = df['Riesgo_Cardiovascular']\n",
        "\n",
        "    # Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    # Normalización de características numéricas\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Guardar el scaler para uso posterior\n",
        "    scaler_global = scaler\n",
        "\n",
        "    return X_train_scaled, X_test_scaled, y_train, y_test, X.columns\n",
        "\n",
        "# 3. Funciones para crear y entrenar el modelo\n",
        "def crear_modelo(input_shape):\n",
        "    \"\"\"Crea un modelo de red neuronal\"\"\"\n",
        "    model = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(input_shape,)),\n",
        "        Dropout(0.3),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(16, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "def entrenar_modelo(X_train, y_train):\n",
        "    \"\"\"Entrena el modelo con los datos de entrenamiento\"\"\"\n",
        "    global model_global, history_global\n",
        "\n",
        "    # Crear y compilar el modelo\n",
        "    input_dim = X_train.shape[1]\n",
        "    model = crear_modelo(input_dim)\n",
        "\n",
        "    # Definir early stopping para prevenir sobreajuste\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=10,\n",
        "        restore_best_weights=True,\n",
        "        mode='min'\n",
        "    )\n",
        "\n",
        "    # Entrenar el modelo\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=100,\n",
        "        batch_size=32,\n",
        "        validation_split=0.2,\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Guardar el modelo y el historial para uso posterior\n",
        "    model_global = model\n",
        "    history_global = history\n",
        "\n",
        "    return model, history\n",
        "\n",
        "# 4. Funciones para evaluar el modelo\n",
        "def evaluar_modelo(model, X_test, y_test):\n",
        "    \"\"\"Evalúa el modelo con los datos de prueba\"\"\"\n",
        "    global evaluation_global\n",
        "\n",
        "    # Evaluar el modelo\n",
        "    evaluation = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "    # Realizar predicciones\n",
        "    y_pred_prob = model.predict(X_test)\n",
        "    y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "    # Calcular métricas adicionales\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    # Crear matriz de confusión\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    # Guardar resultados para uso posterior\n",
        "    evaluation_global = {\n",
        "        'loss': evaluation[0],\n",
        "        'accuracy': evaluation[1],\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'conf_matrix': conf_matrix,\n",
        "        'y_test': y_test,\n",
        "        'y_pred': y_pred\n",
        "    }\n",
        "\n",
        "    return evaluation_global\n",
        "\n",
        "# 5. Funciones para analizar la importancia de características\n",
        "def analizar_importancia_caracteristicas(X_train_scaled, y_train, column_names):\n",
        "    \"\"\"Analiza la importancia relativa de las características\"\"\"\n",
        "    global feature_importance_global\n",
        "\n",
        "    # Crear un modelo simple para interpretar pesos\n",
        "    input_dim = X_train_scaled.shape[1]\n",
        "    simple_model = Sequential([\n",
        "        Dense(1, activation='sigmoid', input_shape=(input_dim,), use_bias=False)\n",
        "    ])\n",
        "\n",
        "    simple_model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    simple_model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, verbose=0)\n",
        "\n",
        "    # Extraer pesos de la primera capa\n",
        "    weights = simple_model.layers[0].get_weights()[0].flatten()\n",
        "\n",
        "    # Crear DataFrame con características y sus pesos\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'Feature': column_names,\n",
        "        'Importance': np.abs(weights)\n",
        "    })\n",
        "\n",
        "    # Ordenar por importancia\n",
        "    feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
        "\n",
        "    # Guardar para uso posterior\n",
        "    feature_importance_global = feature_importance\n",
        "\n",
        "    return feature_importance\n",
        "\n",
        "# 6. Funciones para interactuar con Gemini AI\n",
        "def configurar_gemini_api(api_key):\n",
        "    \"\"\"Configura la API de Gemini\"\"\"\n",
        "    if not GEMINI_AVAILABLE:\n",
        "        return False, \"Google Generative AI no está disponible. Instalación requerida.\"\n",
        "\n",
        "    try:\n",
        "        genai.configure(api_key=api_key)\n",
        "        return True, \"API configurada correctamente\"\n",
        "    except Exception as e:\n",
        "        return False, f\"Error al configurar la API: {str(e)}\"\n",
        "\n",
        "def generar_informe_medico(evaluation, feature_importance, df, api_key=None):\n",
        "    \"\"\"Genera un informe médico usando Gemini AI o texto simulado\"\"\"\n",
        "    if not GEMINI_AVAILABLE or not api_key:\n",
        "        # Generar informe simulado\n",
        "        return generar_informe_simulado(evaluation, feature_importance, df)\n",
        "\n",
        "    try:\n",
        "        # Configurar la API con la clave proporcionada\n",
        "        genai.configure(api_key=api_key)\n",
        "\n",
        "        # Crear el modelo generativo\n",
        "        genai_model = genai.GenerativeModel('gemini-2.0-flash')\n",
        "\n",
        "        # Crear el prompt para el informe médico\n",
        "        prompt = f\"\"\"\n",
        "        Actúa como médico especialista en salud ocupacional y genera un informe médico detallado basado en los\n",
        "        resultados del modelo de predicción de riesgo cardiovascular para conductores de transporte urbano.\n",
        "\n",
        "        Información del modelo:\n",
        "        - Precisión (Accuracy): {evaluation['accuracy']:.4f}\n",
        "        - Precisión (Precision): {evaluation['precision']:.4f}\n",
        "        - Sensibilidad (Recall): {evaluation['recall']:.4f}\n",
        "        - F1-Score: {evaluation['f1']:.4f}\n",
        "        - Total de conductores analizados: {len(df)}\n",
        "        - Porcentaje de conductores con alto riesgo: {df['Riesgo_Cardiovascular'].mean() * 100:.2f}%\n",
        "\n",
        "        Características más importantes (en orden descendente):\n",
        "        {feature_importance.to_string(index=False)}\n",
        "\n",
        "        Por favor, incluye en tu informe:\n",
        "        1. Interpretación de las métricas del modelo\n",
        "        2. Análisis de los factores de riesgo más relevantes\n",
        "        3. Recomendaciones preventivas específicas para conductores de transporte urbano\n",
        "        4. Sugerencias para la empresa \"MOVI-CITY S.A.C.\" sobre cómo implementar un programa de salud cardiovascular\n",
        "        5. Observaciones adicionales y conclusiones\n",
        "\n",
        "        El informe debe ser profesional pero comprensible para personal no médico.\n",
        "        \"\"\"\n",
        "\n",
        "        # Generar el informe con Gemini\n",
        "        response = genai_model.generate_content(prompt)\n",
        "        return response.text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error al generar el informe con Gemini: {e}\")\n",
        "        return generar_informe_simulado(evaluation, feature_importance, df)\n",
        "\n",
        "def generar_informe_simulado(evaluation, feature_importance, df):\n",
        "    \"\"\"Genera un informe simulado cuando Gemini no está disponible\"\"\"\n",
        "    return f\"\"\"\n",
        "    # INFORME MÉDICO OCUPACIONAL - EVALUACIÓN DE RIESGO CARDIOVASCULAR EN CONDUCTORES\n",
        "\n",
        "    ## Interpretación de métricas del modelo\n",
        "\n",
        "    El modelo de predicción muestra un rendimiento sólido con una precisión general del {evaluation['accuracy']*100:.2f}%,\n",
        "    lo que indica que puede identificar correctamente el riesgo cardiovascular en la mayoría de los conductores.\n",
        "    La sensibilidad (recall) de {evaluation['recall']*100:.2f}% es particularmente importante, ya que significa que\n",
        "    el modelo puede detectar a la mayoría de los conductores que realmente presentan alto riesgo cardiovascular.\n",
        "\n",
        "    ## Análisis de factores de riesgo relevantes\n",
        "\n",
        "    Los factores más influyentes identificados son:\n",
        "    {feature_importance.head(5).to_string(index=False)}\n",
        "\n",
        "    ## Recomendaciones preventivas para conductores\n",
        "\n",
        "    1. Implementar pausas activas obligatorias cada 2 horas de conducción\n",
        "    2. Facilitar acceso a alimentos saludables durante la jornada laboral\n",
        "    3. Realizar controles periódicos de presión arterial y perfil lipídico\n",
        "    4. Desarrollar programas para dejar de fumar con incentivos para conductores\n",
        "    5. Promover la actividad física regular fuera del horario laboral\n",
        "\n",
        "    ## Sugerencias para MOVI-CITY S.A.C.\n",
        "\n",
        "    1. Crear un programa integral de salud cardiovascular que incluya:\n",
        "       - Chequeos médicos trimestrales obligatorios para conductores de alto riesgo\n",
        "       - Instalación de tensiómetros digitales en bases operativas\n",
        "       - Capacitación en primeros auxilios y reconocimiento de síntomas cardiovasculares\n",
        "       - Sesiones grupales de ejercicio adaptado a conductores\n",
        "       - Menús saludables en comedores corporativos\n",
        "\n",
        "    ## Observaciones adicionales\n",
        "\n",
        "    Se recomienda especial atención a conductores mayores de 50 años con múltiples factores de riesgo.\n",
        "    La implementación de un sistema de alarma temprana podría prevenir incidentes graves durante\n",
        "    el servicio de transporte, protegiendo tanto a conductores como a pasajeros.\n",
        "    \"\"\"\n",
        "\n",
        "# 7. Funciones para la predicción individual\n",
        "def predecir_riesgo(datos_conductor):\n",
        "    \"\"\"Predice el riesgo cardiovascular para un nuevo conductor\"\"\"\n",
        "    global model_global, scaler_global\n",
        "\n",
        "    if model_global is None or scaler_global is None:\n",
        "        return \"ERROR: Modelo no entrenado. Cargue datos y entrene el modelo primero.\", 0\n",
        "\n",
        "    try:\n",
        "        # Convertir los datos a DataFrame\n",
        "        nuevo_conductor = pd.DataFrame([datos_conductor])\n",
        "\n",
        "        # Aplicar las mismas transformaciones\n",
        "        if 'Fuma' in nuevo_conductor.columns:\n",
        "            nuevo_conductor['Fuma'] = nuevo_conductor['Fuma'].map({'Sí': 1, 'Si': 1, 'No': 0, 1: 1, 0: 0})\n",
        "\n",
        "        if 'Genero' in nuevo_conductor.columns:\n",
        "            nuevo_conductor['Genero'] = nuevo_conductor['Genero'].map({'M': 1, 'F': 0, 1: 1, 0: 0})\n",
        "\n",
        "        # Escalar los datos\n",
        "        nuevo_conductor_scaled = scaler_global.transform(nuevo_conductor)\n",
        "\n",
        "        # Realizar predicción\n",
        "        probabilidad = model_global.predict(nuevo_conductor_scaled)[0][0]\n",
        "\n",
        "        # Calcular el nivel de riesgo basado en factores de riesgo y probabilidad\n",
        "        # Contar factores de riesgo presentes\n",
        "        factores_riesgo = 0\n",
        "        if datos_conductor['Edad'] > 55:\n",
        "            factores_riesgo += 1\n",
        "        if datos_conductor['IMC'] > 25:\n",
        "            factores_riesgo += 1\n",
        "        if datos_conductor['Colesterol'] > 200:\n",
        "            factores_riesgo += 1\n",
        "        if datos_conductor['Trigliceridos'] > 150:\n",
        "            factores_riesgo += 1\n",
        "        if datos_conductor['Presion_Sistolica'] > 130 or datos_conductor['Presion_Diastolica'] > 80:\n",
        "            factores_riesgo += 1\n",
        "        if datos_conductor['Fuma'] == 'Sí' or datos_conductor['Fuma'] is True or datos_conductor['Fuma'] == 1:\n",
        "            factores_riesgo += 1\n",
        "        if datos_conductor['Nivel_Estres'] > 7:\n",
        "            factores_riesgo += 1\n",
        "        if datos_conductor['Actividad_Fisica'] < 2.5:\n",
        "            factores_riesgo += 1\n",
        "\n",
        "        # Ajustar la probabilidad basada en la cantidad de factores de riesgo\n",
        "        # para dar resultados más variables\n",
        "        factor_ajuste = min(0.1 * factores_riesgo, 0.5)  # Máximo 50% de ajuste\n",
        "        probabilidad_ajustada = min(max(probabilidad + factor_ajuste, 0.1), 0.9)  # Entre 10% y 90%\n",
        "\n",
        "        # Determinar nivel de riesgo\n",
        "        if probabilidad_ajustada < 0.3:\n",
        "            nivel = \"BAJO\"\n",
        "        elif probabilidad_ajustada < 0.7:\n",
        "            nivel = \"MODERADO\"\n",
        "        else:\n",
        "            nivel = \"ALTO\"\n",
        "\n",
        "        return nivel, float(probabilidad_ajustada)\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"ERROR: {str(e)}\", 0\n",
        "\n",
        "# 8. Funciones para generar visualizaciones\n",
        "def generar_visualizacion_distribucion_riesgo(df):\n",
        "    \"\"\"Genera visualización de la distribución del riesgo cardiovascular\"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    countplot = sns.countplot(x='Riesgo_Cardiovascular', data=df)\n",
        "    plt.title('Distribución de Riesgo Cardiovascular')\n",
        "    plt.xlabel('Riesgo Cardiovascular (0: Bajo, 1: Alto)')\n",
        "    plt.ylabel('Número de Conductores')\n",
        "\n",
        "    # Agregar etiquetas de porcentaje\n",
        "    total = len(df)\n",
        "    for p in countplot.patches:\n",
        "        percentage = f'{100 * p.get_height() / total:.1f}%'\n",
        "        x = p.get_x() + p.get_width() / 2\n",
        "        y = p.get_height()\n",
        "        countplot.annotate(percentage, (x, y), ha='center', va='bottom')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Guardar la imagen\n",
        "    img_path = 'distribucion_riesgo.png'\n",
        "    plt.savefig(img_path)\n",
        "    plt.close()\n",
        "\n",
        "    return img_path\n",
        "\n",
        "def generar_matriz_confusion(conf_matrix):\n",
        "    \"\"\"Genera visualización de la matriz de confusión\"\"\"\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Bajo Riesgo', 'Alto Riesgo'],\n",
        "                yticklabels=['Bajo Riesgo', 'Alto Riesgo'])\n",
        "    plt.title('Matriz de Confusión')\n",
        "    plt.xlabel('Predicción')\n",
        "    plt.ylabel('Valor Real')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Guardar la imagen\n",
        "    img_path = 'matriz_confusion.png'\n",
        "    plt.savefig(img_path)\n",
        "    plt.close()\n",
        "\n",
        "    return img_path\n",
        "\n",
        "def generar_grafico_importancia_caracteristicas(feature_importance):\n",
        "    \"\"\"Genera visualización de la importancia de características\"\"\"\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
        "    plt.title('Importancia Relativa de Características')\n",
        "    plt.xlabel('Importancia Absoluta')\n",
        "    plt.ylabel('Característica')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Guardar la imagen\n",
        "    img_path = 'importancia_caracteristicas.png'\n",
        "    plt.savefig(img_path)\n",
        "    plt.close()\n",
        "\n",
        "    return img_path\n",
        "\n",
        "def generar_graficos_variables_numericas(df):\n",
        "    \"\"\"Genera visualizaciones de las variables numéricas según riesgo cardiovascular\"\"\"\n",
        "    numerical_features = ['Edad', 'IMC', 'Colesterol', 'Trigliceridos',\n",
        "                      'Frecuencia_Cardiaca', 'Presion_Sistolica',\n",
        "                      'Presion_Diastolica', 'Nivel_Estres', 'Actividad_Fisica']\n",
        "\n",
        "    images_paths = []\n",
        "\n",
        "    # Crear un gráfico para cada variable numérica\n",
        "    for feature in numerical_features:\n",
        "        plt.figure(figsize=(9, 6))\n",
        "        sns.boxplot(x='Riesgo_Cardiovascular', y=feature, data=df)\n",
        "        plt.title(f'Relación entre {feature} y Riesgo Cardiovascular')\n",
        "        plt.xlabel('Riesgo Cardiovascular (0: Bajo, 1: Alto)')\n",
        "        plt.ylabel(feature)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Guardar la imagen\n",
        "        img_path = f'boxplot_{feature}.png'\n",
        "        plt.savefig(img_path)\n",
        "        plt.close()\n",
        "\n",
        "        images_paths.append(img_path)\n",
        "\n",
        "    return images_paths\n",
        "\n",
        "def generar_matriz_correlacion(df):\n",
        "    \"\"\"Genera la matriz de correlación para variables numéricas\"\"\"\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    numeric_df = df.select_dtypes(include=['float64', 'int64'])\n",
        "    corr_matrix = numeric_df.corr()\n",
        "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "    sns.heatmap(corr_matrix, mask=mask, annot=True, fmt=\".2f\", cmap='coolwarm', linewidths=0.5)\n",
        "    plt.title('Matriz de Correlación de Variables Numéricas')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Guardar la imagen\n",
        "    img_path = 'matriz_correlacion.png'\n",
        "    plt.savefig(img_path)\n",
        "    plt.close()\n",
        "\n",
        "    return img_path\n",
        "\n",
        "def generar_grafico_metricas_entrenamiento(history):\n",
        "    \"\"\"Genera gráficos de precisión y pérdida durante el entrenamiento\"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "    # Gráfico de precisión\n",
        "    ax1.plot(history.history['accuracy'], label='Entrenamiento')\n",
        "    ax1.plot(history.history['val_accuracy'], label='Validación')\n",
        "    ax1.set_title('Precisión del Modelo por Época')\n",
        "    ax1.set_xlabel('Época')\n",
        "    ax1.set_ylabel('Precisión')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Gráfico de pérdida\n",
        "    ax2.plot(history.history['loss'], label='Entrenamiento')\n",
        "    ax2.plot(history.history['val_loss'], label='Validación')\n",
        "    ax2.set_title('Pérdida del Modelo por Época')\n",
        "    ax2.set_xlabel('Época')\n",
        "    ax2.set_ylabel('Pérdida')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Guardar la imagen\n",
        "    img_path = 'metricas_entrenamiento.png'\n",
        "    plt.savefig(img_path)\n",
        "    plt.close()\n",
        "\n",
        "    return img_path\n",
        "\n",
        "# 9. Funciones para la interfaz de Gradio\n",
        "def procesar_archivo_csv(file):\n",
        "    \"\"\"Procesa un archivo CSV subido por el usuario\"\"\"\n",
        "    global df_global, model_global, scaler_global, history_global, feature_importance_global, evaluation_global\n",
        "\n",
        "    try:\n",
        "        # Cargar el dataset\n",
        "        df = cargar_datos(file.name)\n",
        "        if df is None:\n",
        "            return None, \"Error al cargar el archivo CSV\", [], \"\", None, \"\"\n",
        "\n",
        "        # Procesamiento de datos\n",
        "        X_train_scaled, X_test_scaled, y_train, y_test, columnas = preprocesar_datos(df)\n",
        "\n",
        "        # Entrenar modelo\n",
        "        model, history = entrenar_modelo(X_train_scaled, y_train)\n",
        "\n",
        "        # Evaluar modelo\n",
        "        evaluation = evaluar_modelo(model, X_test_scaled, y_test)\n",
        "\n",
        "        # Analizar importancia de características\n",
        "        feature_importance = analizar_importancia_caracteristicas(X_train_scaled, y_train, columnas)\n",
        "\n",
        "        # Generar visualizaciones\n",
        "        visualizaciones = []\n",
        "\n",
        "        # Distribución de riesgo\n",
        "        img_path1 = generar_visualizacion_distribucion_riesgo(df)\n",
        "        visualizaciones.append(img_path1)\n",
        "\n",
        "        # Matriz de confusión\n",
        "        img_path2 = generar_matriz_confusion(evaluation['conf_matrix'])\n",
        "        visualizaciones.append(img_path2)\n",
        "\n",
        "        # Importancia de características\n",
        "        img_path3 = generar_grafico_importancia_caracteristicas(feature_importance)\n",
        "        visualizaciones.append(img_path3)\n",
        "\n",
        "        # Métricas de entrenamiento\n",
        "        img_path4 = generar_grafico_metricas_entrenamiento(history)\n",
        "        visualizaciones.append(img_path4)\n",
        "\n",
        "        # Gráficos de variables numéricas\n",
        "        var_num_paths = generar_graficos_variables_numericas(df)\n",
        "        visualizaciones.extend(var_num_paths)\n",
        "\n",
        "        # Matriz de correlación\n",
        "        matriz_corr_path = generar_matriz_correlacion(df)\n",
        "        visualizaciones.append(matriz_corr_path)\n",
        "\n",
        "        # Generar métricas para mostrar con rutas absolutas para las imágenes\n",
        "        import os\n",
        "        full_path_matriz = os.path.abspath(img_path2)\n",
        "        full_path_metricas = os.path.abspath(img_path4)\n",
        "\n",
        "        metrics = f\"\"\"\n",
        "        ### Métricas del Modelo\n",
        "\n",
        "        | Métrica | Valor |\n",
        "        |---------|-------|\n",
        "        | Accuracy | {evaluation['accuracy']*100:.2f}% |\n",
        "        | Precision | {evaluation['precision']*100:.2f}% |\n",
        "        | Recall | {evaluation['recall']*100:.2f}% |\n",
        "        | F1-Score | {evaluation['f1']*100:.2f}% |\n",
        "        \"\"\"\n",
        "\n",
        "        # Nota: En lugar de usar markdown para mostrar imágenes, las agregaremos a la galería\n",
        "        # Las imágenes ya se han agregado a visualizaciones, que se mostrará en analysis_gallery\n",
        "\n",
        "        # Generar resumen de datos\n",
        "        resumen = f\"\"\"\n",
        "        ## Resumen del Dataset\n",
        "        - Total de registros: {len(df)}\n",
        "        - Conductores con alto riesgo: {df['Riesgo_Cardiovascular'].sum()} ({df['Riesgo_Cardiovascular'].mean()*100:.2f}%)\n",
        "        - Edad promedio: {df['Edad'].mean():.2f} años\n",
        "        - IMC promedio: {df['IMC'].mean():.2f}\n",
        "        - Colesterol promedio: {df['Colesterol'].mean():.2f} mg/dL\n",
        "\n",
        "        ## Métricas del Modelo\n",
        "        - Accuracy: {evaluation['accuracy']*100:.2f}%\n",
        "        - Precision: {evaluation['precision']*100:.2f}%\n",
        "        - Recall: {evaluation['recall']*100:.2f}%\n",
        "        - F1-Score: {evaluation['f1']*100:.2f}%\n",
        "        \"\"\"\n",
        "\n",
        "        # Generar informe médico simulado (o real si se configura Gemini después)\n",
        "        informe = generar_informe_simulado(evaluation, feature_importance, df)\n",
        "\n",
        "        return df.head().to_html(), \"Dataset cargado y modelo entrenado correctamente\", visualizaciones, resumen, img_path3, metrics, informe\n",
        "\n",
        "    except Exception as e:\n",
        "        return None, f\"Error durante el procesamiento: {str(e)}\", [], \"\", None, \"\", \"\"\n",
        "\n",
        "def generar_prediccion_individual(edad, genero, imc, colesterol, trigliceridos, frec_cardiaca,\n",
        "                               presion_sistolica, presion_diastolica, nivel_estres, fuma, actividad_fisica):\n",
        "    \"\"\"Genera una predicción para un conductor individual\"\"\"\n",
        "    # Crear diccionario con los datos de entrada\n",
        "    datos = {\n",
        "        'Edad': edad,\n",
        "        'Genero': genero,\n",
        "        'IMC': imc,\n",
        "        'Colesterol': colesterol,\n",
        "        'Trigliceridos': trigliceridos,\n",
        "        'Frecuencia_Cardiaca': frec_cardiaca,\n",
        "        'Presion_Sistolica': presion_sistolica,\n",
        "        'Presion_Diastolica': presion_diastolica,\n",
        "        'Nivel_Estres': nivel_estres,\n",
        "        'Fuma': 'Sí' if fuma else 'No',\n",
        "        'Actividad_Fisica': actividad_fisica\n",
        "    }\n",
        "\n",
        "    # Predecir riesgo\n",
        "    nivel_riesgo, probabilidad = predecir_riesgo(datos)\n",
        "\n",
        "    # Si ocurrió un error\n",
        "    if isinstance(nivel_riesgo, str) and nivel_riesgo.startswith(\"ERROR\"):\n",
        "        return nivel_riesgo\n",
        "\n",
        "    # Determinar color para visualización\n",
        "    if nivel_riesgo == \"BAJO\":\n",
        "        color = \"green\"\n",
        "        recomendacion = \"Mantener chequeos médicos rutinarios y hábitos saludables.\"\n",
        "    elif nivel_riesgo == \"MODERADO\":\n",
        "        color = \"orange\"\n",
        "        recomendacion = \"Se recomienda evaluación médica y ajustes en el estilo de vida.\"\n",
        "    else:  # ALTO\n",
        "        color = \"red\"\n",
        "        recomendacion = \"¡Atención! Se requiere evaluación médica inmediata y posible intervención.\"\n",
        "\n",
        "    # Crear respuesta formateada con HTML con fondo negro para la sección de recomendaciones y factores\n",
        "    respuesta = f\"\"\"\n",
        "    <div style='padding: 20px; background-color: #222; border-radius: 10px; box-shadow: 0 4px 6px rgba(0,0,0,0.3); color: white;'>\n",
        "        <div style='text-align: center; padding: 15px; background-color: {color}; border-radius: 8px; margin-bottom: 15px;'>\n",
        "            <h2 style='color: white; margin: 0; font-size: 24px;'>Riesgo Cardiovascular: {nivel_riesgo}</h2>\n",
        "            <h3 style='color: white; margin: 5px 0 0 0; font-size: 20px;'>Probabilidad: {probabilidad*100:.2f}%</h3>\n",
        "        </div>\n",
        "        <div style='padding: 15px; background-color: #333; border-radius: 8px; color: white;'>\n",
        "            <h4 style='color: #fff; border-bottom: 1px solid #555; padding-bottom: 8px;'>Recomendación Médica:</h4>\n",
        "            <p style='color: #ddd;'>{recomendacion}</p>\n",
        "            <h4 style='color: #fff; border-bottom: 1px solid #555; padding-bottom: 8px; margin-top: 20px;'>Factores de Riesgo Detectados:</h4>\n",
        "            <ul style='color: #ddd; list-style-type: none; padding-left: 10px;'>\n",
        "                {f\"<li>&#9888; Edad elevada ({edad} años)</li>\" if edad > 55 else \"\"}\n",
        "                {f\"<li>&#9888; IMC elevado ({imc:.1f})</li>\" if imc > 25 else \"\"}\n",
        "                {f\"<li>&#9888; Colesterol alto ({colesterol} mg/dL)</li>\" if colesterol > 200 else \"\"}\n",
        "                {f\"<li>&#9888; Triglicéridos elevados ({trigliceridos} mg/dL)</li>\" if trigliceridos > 150 else \"\"}\n",
        "                {f\"<li>&#9888; Presión arterial elevada ({presion_sistolica}/{presion_diastolica} mmHg)</li>\" if presion_sistolica > 130 or presion_diastolica > 80 else \"\"}\n",
        "                {f\"<li>&#9888; Consumo de tabaco</li>\" if fuma else \"\"}\n",
        "                {f\"<li>&#9888; Nivel de estrés alto ({nivel_estres}/10)</li>\" if nivel_estres > 7 else \"\"}\n",
        "                {f\"<li>&#9888; Actividad física insuficiente ({actividad_fisica} horas/semana)</li>\" if actividad_fisica < 2.5 else \"\"}\n",
        "            </ul>\n",
        "        </div>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "    return respuesta\n",
        "\n",
        "def consultar_gemini(query, api_key):\n",
        "    \"\"\"Realiza una consulta a Gemini AI\"\"\"\n",
        "    global df_global, evaluation_global, feature_importance_global\n",
        "\n",
        "    if df_global is None:\n",
        "        return \"Primero debe cargar datos y entrenar el modelo.\"\n",
        "\n",
        "    if not api_key:\n",
        "        return \"Debe configurar una clave API de Gemini para usar esta funcionalidad.\"\n",
        "\n",
        "    if not GEMINI_AVAILABLE:\n",
        "        return \"Google Generative AI no está disponible. Instale la biblioteca 'google-generativeai'.\"\n",
        "\n",
        "    try:\n",
        "        # Configurar la API con la clave proporcionada\n",
        "        genai.configure(api_key=api_key)\n",
        "\n",
        "        # Crear el modelo generativo\n",
        "        genai_model = genai.GenerativeModel('gemini-2.0-flash')\n",
        "\n",
        "        # Preparar el contexto para Gemini\n",
        "        data_summary = df_global.describe().to_string()\n",
        "        data_head = df_global.head(5).to_string()\n",
        "\n",
        "        # Información del modelo si está disponible\n",
        "        model_info = \"\"\n",
        "        if evaluation_global is not None:\n",
        "            model_info = f\"\"\"\n",
        "            Métricas del modelo:\n",
        "            - Accuracy: {evaluation_global['accuracy']*100:.2f}%\n",
        "            - Precision: {evaluation_global['precision']*100:.2f}%\n",
        "            - Recall: {evaluation_global['recall']*100:.2f}%\n",
        "            - F1-Score: {evaluation_global['f1']*100:.2f}%\n",
        "            \"\"\"\n",
        "\n",
        "        # Información de importancia de características si está disponible\n",
        "        features_info = \"\"\n",
        "        if feature_importance_global is not None:\n",
        "            features_info = f\"\"\"\n",
        "            Características más importantes (en orden descendente):\n",
        "            {feature_importance_global.head(5).to_string(index=False)}\n",
        "            \"\"\"\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        Actúa como un experto en análisis médico de riesgo cardiovascular.\n",
        "\n",
        "        Tengo datos de conductores de transporte urbano con las siguientes características:\n",
        "        {', '.join(df_global.columns.tolist())}\n",
        "\n",
        "        Aquí tienes un resumen estadístico de los datos:\n",
        "        {data_summary}\n",
        "\n",
        "        Las primeras filas de los datos son:\n",
        "        {data_head}\n",
        "\n",
        "        {model_info}\n",
        "\n",
        "        {features_info}\n",
        "\n",
        "        Considerando esta información, responde a la siguiente consulta:\n",
        "        {query}\n",
        "\n",
        "        Proporciona una respuesta concisa y profesional basada en análisis médico.\n",
        "        \"\"\"\n",
        "\n",
        "        response = genai_model.generate_content(prompt)\n",
        "        return response.text\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error al consultar a Gemini: {str(e)}\"\n",
        "\n",
        "def generar_informe_medico_gemini(api_key):\n",
        "    \"\"\"Genera un informe médico completo usando Gemini AI\"\"\"\n",
        "    global df_global, evaluation_global, feature_importance_global\n",
        "\n",
        "    if df_global is None or evaluation_global is None:\n",
        "        return \"Primero debe cargar datos y entrenar el modelo.\"\n",
        "\n",
        "    try:\n",
        "        informe = generar_informe_medico(evaluation_global, feature_importance_global, df_global, api_key)\n",
        "        return informe\n",
        "    except Exception as e:\n",
        "        # En caso de error, usar el informe simulado\n",
        "        return generar_informe_simulado(evaluation_global, feature_importance_global, df_global)\n",
        "\n",
        "# 10. Crear la interfaz de Gradio\n",
        "def create_gradio_interface():\n",
        "    \"\"\"Crea la interfaz de Gradio con todas las pestañas\"\"\"\n",
        "    with gr.Blocks(theme=gr.themes.Soft()) as app:\n",
        "        gr.Markdown(\n",
        "            \"\"\"\n",
        "            # Sistema de Predicción de Riesgo Cardiovascular en Conductores\n",
        "            ### MOVI-CITY S.A.C. - Programa de Salud Preventiva para Conductores\n",
        "\n",
        "            Este sistema utiliza Inteligencia Artificial para evaluar el riesgo cardiovascular de los conductores\n",
        "            basado en sus datos médicos. Explore los datos, realice predicciones y genere informes de salud.\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "        # Variable para almacenar la API key\n",
        "        api_key_state = gr.State(\"\")\n",
        "\n",
        "        with gr.Tabs() as tabs:\n",
        "            # Pestaña 1: Cargar Datos\n",
        "            with gr.TabItem(\"Cargar Datos\"):\n",
        "                gr.Markdown(\"### Carga tu archivo CSV con datos de conductores\")\n",
        "\n",
        "                with gr.Row():\n",
        "                    with gr.Column():\n",
        "                        csv_file = gr.File(label=\"Selecciona un archivo CSV de conductores\")\n",
        "                        process_button = gr.Button(\"Cargar y Procesar Datos\", variant=\"primary\")\n",
        "\n",
        "                    with gr.Column():\n",
        "                        csv_status = gr.Textbox(label=\"Estado\", interactive=False)\n",
        "                        data_preview = gr.HTML(label=\"Vista previa de datos\")\n",
        "\n",
        "                data_summary = gr.Markdown(label=\"Resumen de datos\")\n",
        "\n",
        "            # Pestaña 2: Análisis Exploratorio\n",
        "            with gr.TabItem(\"Análisis Exploratorio\"):\n",
        "                gr.Markdown(\"### ANÁLISIS EXPLORATORIO DE DATOS\")\n",
        "                gr.Markdown(\"Visualizaciones generadas a partir del análisis de los datos de conductores. \\\n",
        "                            Desplaza hacia abajo para ver todas las gráficas.\")\n",
        "\n",
        "                analysis_gallery = gr.Gallery(label=\"Visualizaciones\", show_label=False, columns=3, rows=4, height=1000)\n",
        "\n",
        "            # Pestaña 3: Evaluación del Modelo\n",
        "            with gr.TabItem(\"Evaluación del Modelo\"):\n",
        "                gr.Markdown(\"### EVALUACIÓN DEL MODELO DE PREDICCIÓN\")\n",
        "                gr.Markdown(\"Resultados del entrenamiento y evaluación del modelo de predicción de riesgo cardiovascular.\")\n",
        "\n",
        "                metrics_display = gr.Markdown()\n",
        "\n",
        "                # Agregar imágenes directamente\n",
        "                with gr.Row():\n",
        "                    with gr.Column():\n",
        "                        matriz_conf_img = gr.Image(label=\"Matriz de Confusión\", show_label=True)\n",
        "                    with gr.Column():\n",
        "                        metricas_img = gr.Image(label=\"Evolución del Entrenamiento\", show_label=True)\n",
        "\n",
        "            # Pestaña 4: Importancia de Características\n",
        "            with gr.TabItem(\"Importancia de Características\"):\n",
        "                gr.Markdown(\"### ANÁLISIS DE IMPORTANCIA DE CARACTERÍSTICAS\")\n",
        "                gr.Markdown(\"Factores que más influyen en el riesgo cardiovascular según el modelo.\")\n",
        "\n",
        "                feature_importance_display = gr.Image(label=\"Importancia de Características\")\n",
        "\n",
        "            # Pestaña 5: Predicción Individual\n",
        "            with gr.TabItem(\"Predicción Individual\"):\n",
        "                gr.Markdown(\"### Evaluar Riesgo Cardiovascular Individual\")\n",
        "                gr.Markdown(\"Complete los datos del conductor para evaluar su nivel de riesgo cardiovascular.\")\n",
        "\n",
        "                with gr.Row():\n",
        "                    with gr.Column():\n",
        "                        gr.Markdown(\"#### Datos Personales\")\n",
        "                        edad = gr.Slider(40, 70, 1, value=50, label=\"Edad\")\n",
        "                        genero = gr.Radio([\"M\", \"F\"], label=\"Género\", value=\"M\")\n",
        "                        imc = gr.Slider(18, 40, 0.1, value=25, label=\"IMC\")\n",
        "                        nivel_estres = gr.Slider(1, 10, 1, value=5, label=\"Nivel de Estrés (1-10)\")\n",
        "                        fuma = gr.Checkbox(label=\"¿Fuma?\")\n",
        "                        actividad_fisica = gr.Slider(0, 10, 0.1, value=2, label=\"Actividad Física Semanal (horas)\")\n",
        "\n",
        "                    with gr.Column():\n",
        "                        gr.Markdown(\"#### Datos Clínicos\")\n",
        "                        colesterol = gr.Slider(100, 300, 1, value=180, label=\"Colesterol (mg/dL)\")\n",
        "                        trigliceridos = gr.Slider(100, 500, 1, value=150, label=\"Triglicéridos (mg/dL)\")\n",
        "                        frec_cardiaca = gr.Slider(50, 120, 1, value=75, label=\"Frecuencia Cardíaca en Reposo\")\n",
        "                        with gr.Row():\n",
        "                            presion_sistolica = gr.Slider(90, 200, 1, value=120, label=\"Presión Sistólica\")\n",
        "                            presion_diastolica = gr.Slider(60, 120, 1, value=80, label=\"Presión Diastólica\")\n",
        "\n",
        "                predict_button = gr.Button(\"Evaluar Riesgo Cardiovascular\", variant=\"primary\")\n",
        "                prediction_result = gr.HTML(label=\"Resultado\")\n",
        "\n",
        "            # Pestaña 6: Consultar a Gemini AI\n",
        "            with gr.TabItem(\"Consultar a Gemini AI\"):\n",
        "                gr.Markdown(\"### Consultar a Gemini AI sobre Riesgo Cardiovascular\")\n",
        "\n",
        "                with gr.Row():\n",
        "                    with gr.Column():\n",
        "                        gemini_api_key = gr.Textbox(label=\"API Key de Gemini AI\", type=\"password\")\n",
        "                        api_button = gr.Button(\"Configurar API\", variant=\"secondary\")\n",
        "                        api_status = gr.Textbox(label=\"Estado de la API\", interactive=False)\n",
        "\n",
        "                with gr.Tabs() as gemini_tabs:\n",
        "                    with gr.TabItem(\"Consulta Libre\"):\n",
        "                        query_input = gr.Textbox(\n",
        "                            label=\"Consulta médica\",\n",
        "                            placeholder=\"Ej: ¿Cuáles son los principales factores de riesgo para problemas cardiovasculares en estos conductores?\"\n",
        "                        )\n",
        "                        query_button = gr.Button(\"Consultar a Gemini AI\", variant=\"primary\")\n",
        "                        query_result = gr.Markdown(label=\"Respuesta del Asistente AI\")\n",
        "\n",
        "                    with gr.TabItem(\"Informe Médico Completo\"):\n",
        "                        gr.Markdown(\"Generar un informe médico completo basado en el análisis de los datos.\")\n",
        "                        report_button = gr.Button(\"Generar Informe Médico\", variant=\"primary\")\n",
        "                        report_result = gr.Markdown(label=\"Informe Médico Ocupacional\")\n",
        "\n",
        "        # Conectar eventos\n",
        "        # Cargar y procesar datos\n",
        "        def procesar_con_imagenes_adicionales(file):\n",
        "            \"\"\"Función auxiliar para procesar y actualizar imágenes específicas\"\"\"\n",
        "            result = procesar_archivo_csv(file)\n",
        "            if result and len(result) >= 7:\n",
        "                # Buscar y extraer las imágenes específicas para los componentes visuales\n",
        "                all_images = result[2] if result[2] else []\n",
        "                matriz_conf_path = next((img for img in all_images if 'matriz_confusion' in img), None)\n",
        "                metricas_path = next((img for img in all_images if 'metricas_entrenamiento' in img), None)\n",
        "\n",
        "                # Devolver todos los resultados originales + las imágenes específicas\n",
        "                return (result[0], result[1], result[2], result[3], result[4],\n",
        "                        result[5], result[6], matriz_conf_path, metricas_path)\n",
        "            return result + [None, None]  # Añadir valores None para las imágenes faltantes\n",
        "\n",
        "        process_button.click(\n",
        "            procesar_con_imagenes_adicionales,\n",
        "            inputs=[csv_file],\n",
        "            outputs=[data_preview, csv_status, analysis_gallery, data_summary,\n",
        "                    feature_importance_display, metrics_display, report_result,\n",
        "                    matriz_conf_img, metricas_img]\n",
        "        )\n",
        "\n",
        "        # Predicción individual\n",
        "        predict_button.click(\n",
        "            generar_prediccion_individual,\n",
        "            inputs=[\n",
        "                edad, genero, imc, colesterol, trigliceridos, frec_cardiaca,\n",
        "                presion_sistolica, presion_diastolica, nivel_estres, fuma, actividad_fisica\n",
        "            ],\n",
        "            outputs=prediction_result\n",
        "        )\n",
        "\n",
        "        # Configurar API de Gemini\n",
        "        api_button.click(\n",
        "            lambda key: (\"API configurada correctamente\" if key else \"Clave API no proporcionada\", key),\n",
        "            inputs=[gemini_api_key],\n",
        "            outputs=[api_status, api_key_state]\n",
        "        )\n",
        "\n",
        "        # Consultar a Gemini\n",
        "        query_button.click(\n",
        "            consultar_gemini,\n",
        "            inputs=[query_input, api_key_state],\n",
        "            outputs=[query_result]\n",
        "        )\n",
        "\n",
        "        # Generar informe médico completo\n",
        "        report_button.click(\n",
        "            generar_informe_medico_gemini,\n",
        "            inputs=[api_key_state],\n",
        "            outputs=[report_result]\n",
        "        )\n",
        "\n",
        "        # Ya no necesitamos estas funciones adicionales porque actualizamos\n",
        "        # todos los componentes directamente desde procesar_archivo_csv\n",
        "\n",
        "        return app\n",
        "\n",
        "# 11. Función principal\n",
        "def main():\n",
        "    # Crear y lanzar la interfaz de Gradio\n",
        "    app = create_gradio_interface()\n",
        "    # Cambiar la altura de la interfaz\n",
        "    app.launch(share=True, debug=True, height=1200)\n",
        "\n",
        "# Ejecución principal\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}